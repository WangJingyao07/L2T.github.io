<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="L2T">
  <meta property="og:title" content="L2T"/>
  <meta property="og:description" content="L2T"/>
  <meta property="og:url" content="https://github.com/WangJingyao07/L2T.github.io"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="L2T">
  <meta name="twitter:description" content="L2T">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>L2T</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Learning to Think: Information-Theoretic Reinforcement Fine-Tuning for LLMs</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://scholar.google.com/citations?hl=zh-CN&user=btThEsYAAAAJ" target="_blank">Jingyao Wang<sup>1,2</sup></a>,</span>
                <span class="author-block">
                  <a href="https://scholar.google.com/citations?user=7U1v1vsAAAAJ&hl=zh-CN" target="_blank">Wenwen Qiang<sup>1,2</sup></a>,</span>
                  <span class="author-block">
                    <a href="https://scholar.google.com/citations?user=MXphcYwAAAAJ&hl=zh-CN" target="_blank">Zeen Song<sup>1,2</sup></a>,</span>
                    <span class="author-block">
                      <a href="https://scholar.google.com/citations?user=-lErK1QAAAAJ&hl=zh-CN" target="_blank">Changwen Zheng<sup>1,2</sup></a>,</span>
                      <span class="author-block">
                        <a href="https://scholar.google.com/citations?user=cVDF1tkAAAAJ&hl=zh-CN" target="_blank">Hui Xiong<sup>3</sup></a>,</span>
                  </div>

                  <div class="is-size-5 publication-authors">
                <span class="author-block">
                  <sup>1</sup>Institute of Software Chinese Academy of Sciences,
                  <sup>2</sup>University of the Chinese Academy of Sciences, 
                  <sup>3</sup>Hong Kong University of Science and Technology
                </span>
              </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://arxiv.org/abs/2505.10425" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                    <!-- Supplementary PDF link -->
                    <span class="link-block">
                      <a href="https://arxiv.org/abs/2505.10425" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Supplementary</span>
                    </a>
                  </span>



                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/ML-TASA/L2T" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2505.10425" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>




<!-- Paper abstract --> 
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
          Large language models (LLMs) excel at complex tasks thanks to advances in their reasoning abilities. However, existing methods overlook the trade-off between reasoning effectiveness and efficiency, often encouraging unnecessarily long reasoning chains and wasting tokens. To address this, we propose Learning to Think (L2T), an information-theoretic reinforcement fine-tuning framework for LLMs to make the models achieve optimal reasoning with fewer tokens. Specifically, L2T treats each query-response interaction as a hierarchical session of multiple episodes and proposes a universal dense process reward, i.e., quantifies the episode-wise information gain in parameters, requiring no extra annotations or task-specific evaluators. We propose a method to quickly estimate this reward based on PAC-Bayes bounds and the Fisher information matrix. Theoretical analyses show that it significantly reduces computational complexity with high estimation accuracy. By immediately rewarding each episode's contribution and penalizing excessive updates, L2T optimizes the model via reinforcement learning to maximize the use of each episode and achieve effective updates. Empirical results on various reasoning benchmarks and base models demonstrate the advantage of L2T across different tasks, boosting both reasoning effectiveness and efficiency.  
          </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->


<section class="section" id="Motivation">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3" id="setup">Motivation</h2>
        <div class="content has-text-justified">

          <!-- 第一段正文 -->
          <p>
            Large Language Models (LLMs) have advanced from basic NLP tasks to complex applications such as code generation, web interaction, and personal assistance, largely driven by improvements in reasoning ability. Recent studies show that scaling test-time computation—e.g., generating more tokens during inference—can logarithmically enhance reasoning performance. Building on this, a new class of reasoning models integrates test-time compute scaling with reinforcement learning (RL), achieving state-of-the-art results on challenging benchmarks. These models employ chain-of-thought (CoT) reasoning to maintain logical coherence and explore deeper solution paths, thereby improving accuracy. However, existing approaches still struggle to balance reasoning effectiveness and efficiency. Most rely solely on final outcome rewards, providing no feedback for intermediate reasoning steps. This delayed reward structure encourages unnecessary chain extensions; models tend to think one more step, causing redundant computation and reduced efficiency. Empirical evidence shows that such methods often double token usage compared to what is needed for correct answers. Moreover, while moderate CoT extension helps in complex problems, excessive reasoning can harm accuracy in simpler ones. As task difficulty varies widely, no fixed reasoning length is universally optimal. Therefore, it is essential to design dense process rewards that evaluate each reasoning step’s contribution, enabling models to generate the most informative tokens efficiently while maintaining reasoning quality.  
          </p>

          <div style="display: flex; justify-content: center; gap: 20px; align-items: center;"><img
                  src="static/images/1.png"
                  alt="L2T"
                  width="100%"
                />
          </div>

          <p>

          </p>
          <div class="box" style="background-color: #e0f2ff;">
            <p><strong>Observation</strong></p>
            <p>
              (i) Existing methods may fail to use test-time compute budgets efficiently, leading to wasted resources: both models have on average used more than twice the minimum tokens required. For example, k=16 achieves accuracy comparable to or exceeding sequential generation at k=24 with fewer tokens. (ii) The additional episodes add no new information and instead degrade performance due to context redundancy: for both models, Acc(k) peaks around k=16-20 and then declines as k increases. (iii) The questions of different difficulty tiers prefer different chain lengths: Tier 4 questions tend to benefit from longer chains, whereas Tier 1 questions can achieve correct results with short chains, where excessive reasoning depth may causes a marked accuracy drop (e.g., falls by over 5% at k=20). These findings underscore the limitations of existing methods, which ignore the balance between reasoning effectiveness and efficiency.   
            </p>
          </div>
        </div> <!-- 修复：添加缺失的 closing tag -->
      </div>
    </div>
  </div>
</section>



<section class="section" id="Overview">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3" id="setup">Overview of L2T</h2>
        <div class="content has-text-justified">

          <!-- 第一段正文 -->
          <p>
            To this end, we propose Learning to Think (L2T), an information-theoretic reinforcement fine-tuning framework for large language models (LLMs). At its core, L2T introduces a universal information-theoretic dense process reward that quantifies the information gain in model parameters. This reward comprises two components: (i) a fitting information gain term that guides the model to capture correctness-critical information during each update, and (ii) a compression penalty that prevents over-optimization, thereby preserving computational efficiency. By treating each question-answer pair as a multi-episode session and assigning immediate rewards to each episode, L2T encourages the model to focus on the progress of reasoning rather than the final outcome. This design effectively suppresses redundant reasoning steps and mitigates unnecessary computational overhead.
The reward is agnostic to input formats, label types, and task domains, requiring no additional annotation. Through reinforcement learning, L2T optimizes the LLM (policy) to generate tokens that most contribute to answer correctness at every reasoning step.
</p>
<p>
  Specifically, L2T operates in three stages:
(i) Problem reformulation: each question-answer interaction is reformulated as a hierarchical session composed of multiple episodes, where each episode corresponds to a reasoning segment supporting dense reward computation and optimization;
(ii) Reward design: after each episode, the information-theoretic reward is computed using PAC-Bayes bounds and the Fisher Information Matrix, enabling early termination of unproductive reasoning and balancing effectiveness with efficiency;
(iii) LLM fine-tuning: the LLM is optimized to maximize cumulative reward across tasks via reinforcement learning, ensuring both reasoning accuracy and computational efficiency.

</p>


          <div style="display: flex; justify-content: center; gap: 20px; align-items: center;"><img
                  src="static/images/3.png"
                  alt="L2T"
                  width="85%"
                />
          </div>



  </div>
</section>



<section class="section" id="Motivation">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3" id="setup">Performance</h2>
        <div class="content has-text-justified">

          <div style="display: flex; justify-content: center; gap: 20px; align-items: center;"><img
                  src="static/images/2.png"
                  alt="L2T"
                  width="100%"
                />
          </div>


        </div> <!-- 修复：添加缺失的 closing tag -->
      </div>
    </div>
  </div>
</section>









<!-- Paper poster -->
<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title">Poster</h2>

      <iframe  src="static/pdfs/Poster_NIPS.pdf" width="100%" height="550">
          </iframe>
        
      </div>
    </div>
  </section>
<!--End paper poster -->





<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>
      @article{wang2025learning,
        title={Learning to think: Information-theoretic reinforcement fine-tuning for llms},
        author={Wang, Jingyao and Qiang, Wenwen and Song, Zeen and Zheng, Changwen and Xiong, Hui},
        journal={arXiv preprint arXiv:2505.10425},
        year={2025}
      }
      </code></pre>
    </div>
</section>
<!--End BibTex citation -->



  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
